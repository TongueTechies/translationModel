{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/english-newari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SN</th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>welcome to Ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I am from Urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SN                     en                     new\n",
       "0 NaN       welcome to Ideax          Ideax ए लसकुस​\n",
       "1 NaN         this is a test              थो test ख​\n",
       "2 NaN  wa are tongue techies  जिपि tongue techies ख​\n",
       "3 NaN    this is just a demo           थो demo जक ख​\n",
       "4 NaN     I am from Urlabari    जि उर्लाबारी च्वंम्ह"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['en','new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome to Ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am from Urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>lets go to see Indra Jatra</td>\n",
       "      <td>नु ईन्द्रजात्रा स्वो वोने ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>Lakhe</td>\n",
       "      <td>लशिँ – पुलुकिशि</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>You are like a lakhe</td>\n",
       "      <td>छ लाखे थें चोँ ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Is your beloved well?</td>\n",
       "      <td>छिमी यज्जु म्ह फु ला ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>I am fine</td>\n",
       "      <td>जी म्ह फु ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              en                          new\n",
       "0               welcome to Ideax               Ideax ए लसकुस​\n",
       "1                 this is a test                   थो test ख​\n",
       "2          wa are tongue techies       जिपि tongue techies ख​\n",
       "3            this is just a demo                थो demo जक ख​\n",
       "4             I am from Urlabari         जि उर्लाबारी च्वंम्ह\n",
       "...                          ...                          ...\n",
       "1006  lets go to see Indra Jatra  नु ईन्द्रजात्रा स्वो वोने ।\n",
       "1007                       Lakhe              लशिँ – पुलुकिशि\n",
       "1008        You are like a lakhe             छ लाखे थें चोँ ।\n",
       "1009       Is your beloved well?       छिमी यज्जु म्ह फु ला ?\n",
       "1010                   I am fine                  जी म्ह फु ।\n",
       "\n",
       "[1011 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10807/2664996961.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"en\"] = df[\"en\"].apply(process_text)\n",
      "/tmp/ipykernel_10807/2664996961.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"new\"] = df[\"new\"].apply(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome to ideax</td>\n",
       "      <td>Ideax ए लसकुस​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test</td>\n",
       "      <td>थो test ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wa are tongue techies</td>\n",
       "      <td>जिपि tongue techies ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is just a demo</td>\n",
       "      <td>थो demo जक ख​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am from urlabari</td>\n",
       "      <td>जि उर्लाबारी च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i am from damak</td>\n",
       "      <td>जि दमक च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i am from birtamod</td>\n",
       "      <td>जि बिर्तामोड च्वंम्ह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my name is nishant</td>\n",
       "      <td>जिगू नां निशान्त खः।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>my name is drishya</td>\n",
       "      <td>जिगू नां दृश्य खः।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hello my name is sushan</td>\n",
       "      <td>ज्वजलपा जिगू नां सुशन खः।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        en                        new\n",
       "0         welcome to ideax             Ideax ए लसकुस​\n",
       "1           this is a test                 थो test ख​\n",
       "2    wa are tongue techies     जिपि tongue techies ख​\n",
       "3      this is just a demo              थो demo जक ख​\n",
       "4       i am from urlabari       जि उर्लाबारी च्वंम्ह\n",
       "5          i am from damak             जि दमक च्वंम्ह\n",
       "6       i am from birtamod       जि बिर्तामोड च्वंम्ह\n",
       "7       my name is nishant       जिगू नां निशान्त खः।\n",
       "8       my name is drishya         जिगू नां दृश्य खः।\n",
       "9  hello my name is sushan  ज्वजलपा जिगू नां सुशन खः।"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing(df):\n",
    "    def process_text(text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    df[\"en\"] = df[\"en\"].apply(process_text)\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[०-९]', '', text)\n",
    "        text = re.sub(r'[()#/@;:<>‘+=।?!|,’‘’]\".', '', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    df[\"new\"] = df[\"new\"].apply(clean_text)    \n",
    "    return df\n",
    "data = preprocessing(data)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     0\n",
       "new    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer_nepali = AutoTokenizer.from_pretrained('sakonii/deberta-base-nepali')\n",
    "\n",
    "def newari_tokenizer(sentence):\n",
    "    tokens = tokenizer_nepali.tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'new'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('basic_english')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer(newari_tokenizer)\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:    \n",
    "    for index,data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = df.iterrows()\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "source_vocab = vocab_transform[SRC_LANGUAGE]\n",
    "target_vocab = vocab_transform[TGT_LANGUAGE]\n",
    "torch.save(source_vocab, 'vocabs/english_vocab.pth')\n",
    "torch.save(target_vocab, 'vocabs/newari_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torch.nn import Transformer\n",
    "import torch.nn as nn \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",

    "        self.embedding = nn.Embedding(vocab_size, emb_size) \n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "   \n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], \n",
    "                                               vocab_transform[ln],  \n",
    "                                               tensor_transform) \n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8 \n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 10\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DROP_OUT = 0.1\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM,DROP_OUT)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "     \n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    \n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "split = round(df.shape[0]* split_ratio)\n",
    "train = df.iloc[:split]\n",
    "train_ds = list(zip(train['en'],train['new']))\n",
    "valid = df.iloc[split:]\n",
    "val_ds = list(zip(valid['en'],valid['new']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "accumulation_steps = 5\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    val_los = 0\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    optimizer.zero_grad() \n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)   \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss = loss / accumulation_steps \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i+1) % accumulation_steps == 0:            \n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss = loss / accumulation_steps \n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peshal/drishya/nlpBasics/venv/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.184, Val loss: 1.170, Epoch time = 2.030s\n",
      "Epoch: 2, Train loss: 1.074, Val loss: 1.134, Epoch time = 1.474s\n",
      "Epoch: 3, Train loss: 1.032, Val loss: 1.111, Epoch time = 1.481s\n",
      "Epoch: 4, Train loss: 0.995, Val loss: 1.093, Epoch time = 1.492s\n",
      "Epoch: 5, Train loss: 0.965, Val loss: 1.084, Epoch time = 1.491s\n",
      "Epoch: 6, Train loss: 0.927, Val loss: 1.066, Epoch time = 1.492s\n",
      "Epoch: 7, Train loss: 0.872, Val loss: 1.047, Epoch time = 1.490s\n",
      "Epoch: 8, Train loss: 0.822, Val loss: 1.051, Epoch time = 1.486s\n",
      "Epoch: 9, Train loss: 0.782, Val loss: 1.046, Epoch time = 1.491s\n",
      "Epoch: 10, Train loss: 0.741, Val loss: 1.040, Epoch time = 1.491s\n",
      "Epoch: 11, Train loss: 0.703, Val loss: 1.050, Epoch time = 1.494s\n",
      "Epoch: 12, Train loss: 0.671, Val loss: 1.057, Epoch time = 1.493s\n",
      "Epoch: 13, Train loss: 0.635, Val loss: 1.049, Epoch time = 1.495s\n",
      "Epoch: 14, Train loss: 0.598, Val loss: 1.059, Epoch time = 1.492s\n",
      "Epoch: 15, Train loss: 0.568, Val loss: 1.066, Epoch time = 1.493s\n",
      "Epoch: 16, Train loss: 0.538, Val loss: 1.057, Epoch time = 1.492s\n",
      "Epoch: 17, Train loss: 0.507, Val loss: 1.072, Epoch time = 1.492s\n",
      "Epoch: 18, Train loss: 0.489, Val loss: 1.079, Epoch time = 1.495s\n",
      "Epoch: 19, Train loss: 0.468, Val loss: 1.082, Epoch time = 1.496s\n",
      "Epoch: 20, Train loss: 0.444, Val loss: 1.083, Epoch time = 1.489s\n",
      "Epoch: 21, Train loss: 0.427, Val loss: 1.087, Epoch time = 1.491s\n",
      "Epoch: 22, Train loss: 0.409, Val loss: 1.093, Epoch time = 1.494s\n",
      "Epoch: 23, Train loss: 0.395, Val loss: 1.156, Epoch time = 1.493s\n",
      "Epoch: 24, Train loss: 0.386, Val loss: 1.112, Epoch time = 1.490s\n",
      "Epoch: 25, Train loss: 0.370, Val loss: 1.143, Epoch time = 1.496s\n",
      "Epoch: 26, Train loss: 0.347, Val loss: 1.170, Epoch time = 1.495s\n",
      "Epoch: 27, Train loss: 0.336, Val loss: 1.191, Epoch time = 1.492s\n",
      "Epoch: 28, Train loss: 0.321, Val loss: 1.195, Epoch time = 1.493s\n",
      "Epoch: 29, Train loss: 0.316, Val loss: 1.175, Epoch time = 1.491s\n",
      "Epoch: 30, Train loss: 0.301, Val loss: 1.166, Epoch time = 1.494s\n",
      "Epoch: 31, Train loss: 0.285, Val loss: 1.166, Epoch time = 1.490s\n",
      "Epoch: 32, Train loss: 0.271, Val loss: 1.183, Epoch time = 1.495s\n",
      "Epoch: 33, Train loss: 0.256, Val loss: 1.176, Epoch time = 1.493s\n",
      "Epoch: 34, Train loss: 0.247, Val loss: 1.166, Epoch time = 1.493s\n",
      "Epoch: 35, Train loss: 0.233, Val loss: 1.136, Epoch time = 1.491s\n",
      "Epoch: 36, Train loss: 0.217, Val loss: 1.132, Epoch time = 1.500s\n",
      "Epoch: 37, Train loss: 0.210, Val loss: 1.157, Epoch time = 1.494s\n",
      "Epoch: 38, Train loss: 0.206, Val loss: 1.127, Epoch time = 1.493s\n",
      "Epoch: 39, Train loss: 0.193, Val loss: 1.141, Epoch time = 1.490s\n",
      "Epoch: 40, Train loss: 0.183, Val loss: 1.138, Epoch time = 1.493s\n",
      "Epoch: 41, Train loss: 0.172, Val loss: 1.118, Epoch time = 1.491s\n",
      "Epoch: 42, Train loss: 0.169, Val loss: 1.159, Epoch time = 1.493s\n",
      "Epoch: 43, Train loss: 0.159, Val loss: 1.175, Epoch time = 1.490s\n",
      "Epoch: 44, Train loss: 0.151, Val loss: 1.242, Epoch time = 1.497s\n",
      "Epoch: 45, Train loss: 0.144, Val loss: 1.247, Epoch time = 1.494s\n",
      "Epoch: 46, Train loss: 0.140, Val loss: 1.268, Epoch time = 1.495s\n",
      "Epoch: 47, Train loss: 0.130, Val loss: 1.234, Epoch time = 1.494s\n",
      "Epoch: 48, Train loss: 0.128, Val loss: 1.237, Epoch time = 1.492s\n",
      "Epoch: 49, Train loss: 0.120, Val loss: 1.219, Epoch time = 1.497s\n",
      "Epoch: 50, Train loss: 0.120, Val loss: 1.192, Epoch time = 1.488s\n",
      "Epoch: 51, Train loss: 0.112, Val loss: 1.223, Epoch time = 1.500s\n",
      "Epoch: 52, Train loss: 0.109, Val loss: 1.172, Epoch time = 1.492s\n",
      "Epoch: 53, Train loss: 0.101, Val loss: 1.201, Epoch time = 1.491s\n",
      "Epoch: 54, Train loss: 0.097, Val loss: 1.225, Epoch time = 1.492s\n",
      "Epoch: 55, Train loss: 0.094, Val loss: 1.252, Epoch time = 1.499s\n",
      "Epoch: 56, Train loss: 0.089, Val loss: 1.308, Epoch time = 1.493s\n",
      "Epoch: 57, Train loss: 0.085, Val loss: 1.352, Epoch time = 1.494s\n",
      "Epoch: 58, Train loss: 0.086, Val loss: 1.375, Epoch time = 1.497s\n",
      "Epoch: 59, Train loss: 0.083, Val loss: 1.260, Epoch time = 1.493s\n",
      "Epoch: 60, Train loss: 0.079, Val loss: 1.248, Epoch time = 1.492s\n",
      "Epoch: 61, Train loss: 0.073, Val loss: 1.223, Epoch time = 1.499s\n",
      "Epoch: 62, Train loss: 0.073, Val loss: 1.255, Epoch time = 1.496s\n",
      "Epoch: 63, Train loss: 0.075, Val loss: 1.249, Epoch time = 1.502s\n",
      "Epoch: 64, Train loss: 0.067, Val loss: 1.243, Epoch time = 1.493s\n",
      "Epoch: 65, Train loss: 0.065, Val loss: 1.314, Epoch time = 1.494s\n",
      "Epoch: 66, Train loss: 0.060, Val loss: 1.341, Epoch time = 1.492s\n",
      "Epoch: 67, Train loss: 0.057, Val loss: 1.306, Epoch time = 1.496s\n",
      "Epoch: 68, Train loss: 0.056, Val loss: 1.300, Epoch time = 1.493s\n",
      "Epoch: 69, Train loss: 0.056, Val loss: 1.274, Epoch time = 1.494s\n",
      "Epoch: 70, Train loss: 0.056, Val loss: 1.276, Epoch time = 1.498s\n",
      "Epoch: 71, Train loss: 0.055, Val loss: 1.300, Epoch time = 1.495s\n",
      "Epoch: 72, Train loss: 0.050, Val loss: 1.311, Epoch time = 1.493s\n",
      "Epoch: 73, Train loss: 0.050, Val loss: 1.322, Epoch time = 1.488s\n",
      "Epoch: 74, Train loss: 0.049, Val loss: 1.357, Epoch time = 1.492s\n",
      "Epoch: 75, Train loss: 0.044, Val loss: 1.341, Epoch time = 1.491s\n",
      "Epoch: 76, Train loss: 0.046, Val loss: 1.326, Epoch time = 1.495s\n",
      "Epoch: 77, Train loss: 0.044, Val loss: 1.362, Epoch time = 1.495s\n",
      "Epoch: 78, Train loss: 0.044, Val loss: 1.347, Epoch time = 1.496s\n",
      "Epoch: 79, Train loss: 0.044, Val loss: 1.364, Epoch time = 1.492s\n",
      "Epoch: 80, Train loss: 0.042, Val loss: 1.369, Epoch time = 1.496s\n",
      "Epoch: 81, Train loss: 0.039, Val loss: 1.353, Epoch time = 1.494s\n",
      "Epoch: 82, Train loss: 0.039, Val loss: 1.379, Epoch time = 1.493s\n",
      "Epoch: 83, Train loss: 0.037, Val loss: 1.354, Epoch time = 1.495s\n",
      "Epoch: 84, Train loss: 0.037, Val loss: 1.353, Epoch time = 1.498s\n",
      "Epoch: 85, Train loss: 0.039, Val loss: 1.329, Epoch time = 1.494s\n",
      "Epoch: 86, Train loss: 0.034, Val loss: 1.386, Epoch time = 1.496s\n",
      "Epoch: 87, Train loss: 0.035, Val loss: 1.375, Epoch time = 1.494s\n",
      "Epoch: 88, Train loss: 0.035, Val loss: 1.432, Epoch time = 1.498s\n",
      "Epoch: 89, Train loss: 0.034, Val loss: 1.410, Epoch time = 1.495s\n",
      "Epoch: 90, Train loss: 0.034, Val loss: 1.330, Epoch time = 1.493s\n",
      "Epoch: 91, Train loss: 0.033, Val loss: 1.375, Epoch time = 1.493s\n",
      "Epoch: 92, Train loss: 0.032, Val loss: 1.311, Epoch time = 1.494s\n",
      "Epoch: 93, Train loss: 0.033, Val loss: 1.347, Epoch time = 1.492s\n",
      "Epoch: 94, Train loss: 0.034, Val loss: 1.429, Epoch time = 1.492s\n",
      "Epoch: 95, Train loss: 0.032, Val loss: 1.438, Epoch time = 1.493s\n",
      "Epoch: 96, Train loss: 0.030, Val loss: 1.389, Epoch time = 1.495s\n",
      "Epoch: 97, Train loss: 0.028, Val loss: 1.381, Epoch time = 1.494s\n",
      "Epoch: 98, Train loss: 0.030, Val loss: 1.399, Epoch time = 1.491s\n",
      "Epoch: 99, Train loss: 0.030, Val loss: 1.415, Epoch time = 1.496s\n",
      "Epoch: 100, Train loss: 0.031, Val loss: 1.441, Epoch time = 1.491s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=0.1)\n",
    "NUM_EPOCHS = 100\n",
    "history = {\n",
    "        \"loss\": [], \n",
    "        \"val_los\": []\n",
    "        }\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    history['loss'].append(train_loss)\n",
    "    history['val_los'].append(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(),\"models/english-newari.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ▁गन ▁ख ▁? \n"
     ]
    }
   ],
   "source": [
    "translated_sentence = translate(transformer, \"where are you?\")\n",
    "\n",
    "print(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_subword_tokens(tokens):\n",
    "    tokens = [token.replace(\"▁\", \" \") for token in tokens]\n",
    "    complete_word = \"\".join(tokens)\n",
    "    return complete_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " योको योको शुभाये डाक्टर ।\n"
     ]
    }
   ],
   "source": [
    "def translate_with_postprocessing(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    \n",
    "    tgt_tokens = [token for token in tgt_tokens if token not in [BOS_IDX, EOS_IDX]]\n",
    "    \n",
    "    tgt_tokens = torch.tensor(tgt_tokens)\n",
    "    \n",
    "    translated_sentence = detokenize_subword_tokens(\n",
    "        vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))\n",
    "    )\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "translated_sentence = translate_with_postprocessing(transformer, \"Thank you very much doctor\")\n",
    "print(translated_sentence)\n"
=======
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
